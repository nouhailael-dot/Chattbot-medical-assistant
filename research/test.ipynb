{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd6dfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Langchain/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting medical chatbot setup...\n",
      "Loading PDF documents...\n",
      "Processed 5859 text chunks\n",
      "Initializing embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cf/m0l4l6k95ws066vjj9hwl4t00000gn/T/ipykernel_20958/2105836754.py:44: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  return HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embeddings initialized successfully\n",
      "Initializing Pinecone...\n",
      "✓ Pinecone index already exists\n",
      "Creating vector store...\n",
      "✓ Vector store created\n",
      "Testing retriever...\n",
      "✓ Retriever works - retrieved 3 documents\n",
      "Setting up local LLM...\n",
      "Trying model: microsoft/DialoGPT-medium\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ microsoft/DialoGPT-medium works\n",
      "Testing RAG chain with medical question...\n",
      "\n",
      "Retrieved context snippets:\n",
      "1. Whitehouse Station, NJ: Merck Research Laboratories,\n",
      "1997.\n",
      "Larsen, D. E., ed. Mayo Clinic Family Hea...\n",
      "2. Whitehouse Station, NJ: Merck Research Laboratories,\n",
      "1997.\n",
      "Larsen, D. E., ed. Mayo Clinic Family Hea...\n",
      "3. Whitehouse Station, NJ: Merck Research Laboratories,\n",
      "1997.\n",
      "Larsen, D. E., ed. Mayo Clinic Family Hea...\n",
      "\n",
      "Answer: System: You are a medical expert assistant. Use the retrieved context to answer the question accurately and concisely.\n",
      "\n",
      "IMPORTANT INSTRUCTIONS:\n",
      "1. Use ONLY the provided context to answer the question\n",
      "2. If the context doesn't contain the answer, say \"I don't have enough information about this specific condition\"\n",
      "3. Be factual and precise\n",
      "4. Use simple language that patients can understand\n",
      "5. Focus on key information: symptoms, causes, treatments\n",
      "6. Keep your answer to 2-3 sentences maximum\n",
      "\n",
      "CONTEXT:\n",
      "Whitehouse Station, NJ: Merck Research Laboratories,\n",
      "1997.\n",
      "Larsen, D. E., ed. Mayo Clinic Family Health Book.New York:\n",
      "William Morrow and Co., Inc., 1996.\n",
      "John T. Lohr, PhD\n",
      "Acromegaly and gigantism\n",
      "Definition\n",
      "Acromegaly is a disorder in which the abnormal\n",
      "release of a particular chemical from the pituitary gland\n",
      "in the brain causes increased growth in bone and soft tis-\n",
      "sue, as well as a variety of other disturbances throughout\n",
      "the body. This chemical released from the pituitary gland\n",
      "\n",
      "Whitehouse Station, NJ: Merck Research Laboratories,\n",
      "1997.\n",
      "Larsen, D. E., ed. Mayo Clinic Family Health Book.New York:\n",
      "William Morrow and Co., Inc., 1996.\n",
      "John T. Lohr, PhD\n",
      "Acromegaly and gigantism\n",
      "Definition\n",
      "Acromegaly is a disorder in which the abnormal\n",
      "release of a particular chemical from the pituitary gland\n",
      "in the brain causes increased growth in bone and soft tis-\n",
      "sue, as well as a variety of other disturbances throughout\n",
      "the body. This chemical released from the pituitary gland\n",
      "\n",
      "Whitehouse Station, NJ: Merck Research Laboratories,\n",
      "1997.\n",
      "Larsen, D. E., ed. Mayo Clinic Family Health Book.New York:\n",
      "William Morrow and Co., Inc., 1996.\n",
      "John T. Lohr, PhD\n",
      "Acromegaly and gigantism\n",
      "Definition\n",
      "Acromegaly is a disorder in which the abnormal\n",
      "release of a particular chemical from the pituitary gland\n",
      "in the brain causes increased growth in bone and soft tis-\n",
      "sue, as well as a variety of other disturbances throughout\n",
      "the body. This chemical released from the pituitary gland\n",
      "\n",
      "QUESTION:\n",
      "what is Acromegaly and gigantism?\n",
      "\n",
      "ANSWER:\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY or \"\"\n",
    "\n",
    "def load_pdf_files(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data, \n",
    "        glob=\"*.pdf\", \n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def filter_to_minimal_docs(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Filters the documents to only include those with minimal content.\"\"\"\n",
    "    return [doc for doc in documents if len(doc.page_content.strip()) > 0]\n",
    "\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, \n",
    "        chunk_overlap=20,\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_documents(minimal_docs)\n",
    "\n",
    "def download_embeddings():\n",
    "    \"\"\"Downloads and returns the HuggingFace embeddings model.\"\"\"\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    )\n",
    "\n",
    "def setup_better_local_llm():\n",
    "    \"\"\"Set up a better local LLM for medical questions\"\"\"\n",
    "    try:\n",
    "        # Try to use a more capable local model\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "        from langchain_huggingface import HuggingFacePipeline\n",
    "        import torch\n",
    "        \n",
    "        # Try different models in order of preference\n",
    "        models_to_try = [\n",
    "            \"microsoft/DialoGPT-medium\",    # Good for conversation\n",
    "            \"microsoft/DialoGPT-large\",     # Even better\n",
    "            \"distilgpt2\",                   # Fallback\n",
    "        ]\n",
    "        \n",
    "        for model_name in models_to_try:\n",
    "            try:\n",
    "                print(f\"Trying model: {model_name}\")\n",
    "                \n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "                \n",
    "                # Add padding token if it doesn't exist\n",
    "                if tokenizer.pad_token is None:\n",
    "                    tokenizer.pad_token = tokenizer.eos_token\n",
    "                \n",
    "                pipe = pipeline(\n",
    "                    \"text-generation\",\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_new_tokens=200,\n",
    "                    temperature=0.3,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.1,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
    "                )\n",
    "                \n",
    "                llm = HuggingFacePipeline(pipeline=pipe)\n",
    "                \n",
    "                # Test the model\n",
    "                test_response = llm.invoke(\"Hello, how are you?\")\n",
    "                print(f\"✓ {model_name} works\")\n",
    "                return llm\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ {model_name} failed: {str(e)[:100]}...\")\n",
    "                continue\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Local model setup failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_medical_prompt_template():\n",
    "    \"\"\"Create a better prompt template for medical questions\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a medical expert assistant. Use the retrieved context to answer the question accurately and concisely.\n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "1. Use ONLY the provided context to answer the question\n",
    "2. If the context doesn't contain the answer, say \"I don't have enough information about this specific condition\"\n",
    "3. Be factual and precise\n",
    "4. Use simple language that patients can understand\n",
    "5. Focus on key information: symptoms, causes, treatments\n",
    "6. Keep your answer to 2-3 sentences maximum\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{input}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "    ])\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting medical chatbot setup...\")\n",
    "    \n",
    "    # Load and process documents\n",
    "    print(\"Loading PDF documents...\")\n",
    "    extracted_data = load_pdf_files(\"data\")\n",
    "    minimal_docs = filter_to_minimal_docs(extracted_data)\n",
    "    texts_chunk = text_split(minimal_docs)\n",
    "    print(f\"Processed {len(texts_chunk)} text chunks\")\n",
    "    \n",
    "    # Initialize embeddings\n",
    "    print(\"Initializing embeddings...\")\n",
    "    embeddings = download_embeddings()\n",
    "    print(\"✓ Embeddings initialized successfully\")\n",
    "    \n",
    "    # Initialize Pinecone\n",
    "    print(\"Initializing Pinecone...\")\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    index_name = \"medical-chatbot\"\n",
    "    \n",
    "    # Create index if it doesn't exist\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        print(\"Creating Pinecone index...\")\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=384,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "        )\n",
    "        print(\"✓ Pinecone index created\")\n",
    "    else:\n",
    "        print(\"✓ Pinecone index already exists\")\n",
    "    \n",
    "    # Create vector store\n",
    "    print(\"Creating vector store...\")\n",
    "    docsearch = PineconeVectorStore.from_documents(\n",
    "        documents=texts_chunk,\n",
    "        embedding=embeddings,\n",
    "        index_name=index_name\n",
    "    )\n",
    "    print(\"✓ Vector store created\")\n",
    "    \n",
    "    # Initialize retriever\n",
    "    retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    \n",
    "    # Test retriever\n",
    "    print(\"Testing retriever...\")\n",
    "    retrieved_docs = retriever.invoke(\"What is Acne?\")\n",
    "    print(f\"✓ Retriever works - retrieved {len(retrieved_docs)} documents\")\n",
    "    \n",
    "    # Setup better local LLM\n",
    "    print(\"Setting up local LLM...\")\n",
    "    chat_model = setup_better_local_llm()\n",
    "    \n",
    "    if chat_model is None:\n",
    "        print(\"Using simple echo fallback...\")\n",
    "        from langchain.llms import FakeListLLM\n",
    "        chat_model = FakeListLLM(responses=[\"Based on the medical context, I can provide information about various conditions.\"])\n",
    "    \n",
    "    # Create better medical prompt template\n",
    "    prompt = create_medical_prompt_template()\n",
    "    \n",
    "    # Create chains\n",
    "    question_answer_chain = create_stuff_documents_chain(chat_model, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "    \n",
    "    # Test the chain with better prompting\n",
    "    try:\n",
    "        print(\"Testing RAG chain with medical question...\")\n",
    "        \n",
    "        # First, let's see what context is retrieved\n",
    "        retrieved = retriever.invoke(\"what is Acromegaly and gigantism?\")\n",
    "        print(\"\\nRetrieved context snippets:\")\n",
    "        for i, doc in enumerate(retrieved):\n",
    "            print(f\"{i+1}. {doc.page_content[:100]}...\")\n",
    "        \n",
    "        # Now test the full RAG chain\n",
    "        response = rag_chain.invoke({\"input\": \"what is Acromegaly and gigantism?\"})\n",
    "        print(f\"\\nAnswer: {response['answer']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"RAG chain failed: {e}\")\n",
    "        print(\"Trying manual approach...\")\n",
    "        \n",
    "        # Manual approach with better prompting\n",
    "        retrieved = retriever.invoke(\"what is Acromegaly and gigantism?\")\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved])\n",
    "        \n",
    "        # Create a better manual prompt\n",
    "        manual_prompt = f\"\"\"Based on this medical context: {context}\n",
    "\n",
    "Please answer: what is Acromegaly and gigantism?\n",
    "\n",
    "Focus on:\n",
    "1. What are these conditions?\n",
    "2. What causes them?\n",
    "3. Key symptoms\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            manual_response = chat_model.invoke(manual_prompt)\n",
    "            print(\"Manual response:\", manual_response)\n",
    "        except Exception as e:\n",
    "            print(f\"Manual generation failed: {e}\")\n",
    "            print(\"Showing retrieved context instead:\")\n",
    "            for i, doc in enumerate(retrieved):\n",
    "                print(f\"\\n--- Document {i+1} ---\")\n",
    "                print(doc.page_content[:200] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
